{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b21742af-f7e3-4f4a-9369-3e16c34422eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 20:16:19.461150: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-18 20:16:20.340248: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-18 20:16:20.340336: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-18 20:16:20.479943: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-18 20:16:20.608401: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-18 20:16:22.565821: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import spektral\n",
    "from spektral import transforms as T\n",
    "from spektral import layers\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09f049cc-2bb9-4b15-ad65-0b1b8e247911",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Pack_N_3_device_/job:localhost/replica:0/task:0/device:GPU:0}} Shapes of all inputs must match: values[0].shape = [3] != values[1].shape = [] [Op:Pack] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m     22\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[0;32m---> 23\u001b[0m resized_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mresize_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResized tensor shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, resized_tensor\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m, in \u001b[0;36mresize_tensor\u001b[0;34m(t, h, w)\u001b[0m\n\u001b[1;32m      5\u001b[0m original_shape \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mshape(t)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Compute the dimensions for slicing and padding\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m sliced_shape \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimum\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43moriginal_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m padding \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmaximum([original_shape[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m-\u001b[39m original_shape,h, w, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Slice the input tensor to the desired shape\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/gen_math_ops.py:6613\u001b[0m, in \u001b[0;36mminimum\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6611\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m _result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   6612\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m-> 6613\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mminimum_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   6614\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6615\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[1;32m   6616\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/gen_math_ops.py:6654\u001b[0m, in \u001b[0;36mminimum_eager_fallback\u001b[0;34m(x, y, name, ctx)\u001b[0m\n\u001b[1;32m   6653\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mminimum_eager_fallback\u001b[39m(x: Annotated[Any, TV_Minimum_T], y: Annotated[Any, TV_Minimum_T], name, ctx) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Annotated[Any, TV_Minimum_T]:\n\u001b[0;32m-> 6654\u001b[0m   _attr_T, _inputs_T \u001b[38;5;241m=\u001b[39m \u001b[43m_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs_to_matching_eager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6655\u001b[0m   (x, y) \u001b[38;5;241m=\u001b[39m _inputs_T\n\u001b[1;32m   6656\u001b[0m   _inputs_flat \u001b[38;5;241m=\u001b[39m [x, y]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:267\u001b[0m, in \u001b[0;36margs_to_matching_eager\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    265\u001b[0m       dtype \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m   ret \u001b[38;5;241m=\u001b[39m [tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t, dtype) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m l]\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): consider removing this as it leaks a Keras concept.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    271\u001b[0m keras_symbolic_tensors \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ret \u001b[38;5;28;01mif\u001b[39;00m _is_keras_symbolic_tensor(x)]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:267\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    265\u001b[0m       dtype \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m   ret \u001b[38;5;241m=\u001b[39m [\u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m l]\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): consider removing this as it leaks a Keras concept.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    271\u001b[0m keras_symbolic_tensors \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ret \u001b[38;5;28;01mif\u001b[39;00m _is_keras_symbolic_tensor(x)]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m           _add_error_prefix(\n\u001b[1;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py:1585\u001b[0m, in \u001b[0;36m_autopacking_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype \u001b[38;5;241m!=\u001b[39m inferred_dtype:\n\u001b[1;32m   1584\u001b[0m   v \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(_cast_nested_seqs_to_dtype(dtype), v)\n\u001b[0;32m-> 1585\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_autopacking_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpacked\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py:1521\u001b[0m, in \u001b[0;36m_autopacking_helper\u001b[0;34m(list_or_tuple, dtype, name)\u001b[0m\n\u001b[1;32m   1515\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1516\u001b[0m       \u001b[38;5;66;03m# NOTE(mrry): This is inefficient, but it enables us to\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m       \u001b[38;5;66;03m# handle the case where the list arguments are other\u001b[39;00m\n\u001b[1;32m   1518\u001b[0m       \u001b[38;5;66;03m# convertible-to-tensor types, such as numpy arrays.\u001b[39;00m\n\u001b[1;32m   1519\u001b[0m       elems_as_tensors\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m   1520\u001b[0m           constant_op\u001b[38;5;241m.\u001b[39mconstant(elem, dtype\u001b[38;5;241m=\u001b[39mdtype, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)))\n\u001b[0;32m-> 1521\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpack\u001b[49m\u001b[43m(\u001b[49m\u001b[43melems_as_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m converted_elems\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/gen_array_ops.py:6718\u001b[0m, in \u001b[0;36mpack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   6716\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   6717\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 6718\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6719\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m   6720\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Pack_N_3_device_/job:localhost/replica:0/task:0/device:GPU:0}} Shapes of all inputs must match: values[0].shape = [3] != values[1].shape = [] [Op:Pack] name: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def resize_tensor(t, h, w):\n",
    "    # Get the original shape of the input tensor\n",
    "    original_shape = tf.shape(t)\n",
    "\n",
    "    # Compute the dimensions for slicing and padding\n",
    "    sliced_shape = tf.minimum(original_shape, [original_shape, h, w])\n",
    "    padding = tf.maximum([original_shape[0]] - original_shape,h, w, 0)\n",
    "\n",
    "    # Slice the input tensor to the desired shape\n",
    "    sliced_tensor = tf.slice(t, begin=[0, 0, 0], size=sliced_shape)\n",
    "\n",
    "    # Pad the sliced tensor to the desired shape\n",
    "    padded_tensor = tf.pad(sliced_tensor, paddings=[[0, padding[0]], [0, padding[1]], [0, padding[2]]])\n",
    "\n",
    "    return padded_tensor\n",
    "\n",
    "# Example usage:\n",
    "t = tf.random.normal((5, 7, 3))  # Example input tensor\n",
    "h = 4\n",
    "w = 6\n",
    "resized_tensor = resize_tensor(t, h, w)\n",
    "print(\"Resized tensor shape:\", resized_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd50c43-6a4f-4d13-a5ad-260ebe76b628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6527a6d-d7c2-4bb7-b5b7-b0cceaad5998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not read URL https://chrsmrrs.github.io/datasets/docs/datasets/\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown dataset PROTEINS. See TUDataset.available_datasets() for a complete list ofavailable datasets.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Basic information about the dataset is here: https://paperswithcode.com/dataset/proteins\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# \"clean\" removes isomorphic (non-distinguishable) graphs\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mspektral\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTUDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPROTEINS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#dataset = CustomTUDataset('PROTEINS', clean=True)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#dataset = spektral.datasets.citation.Citation.available_datasets()\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#New in version 1.2: sparse was renamed to sparse_output\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#current version of scikit learn is 1.4.0. I downgraded it to 1.1.3.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spektral/datasets/tudataset.py:60\u001b[0m, in \u001b[0;36mTUDataset.__init__\u001b[0;34m(self, name, clean, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, clean\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailable_datasets():\n\u001b[0;32m---> 60\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     61\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown dataset \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. See \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.available_datasets() for a complete list of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavailable datasets.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     63\u001b[0m         )\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean \u001b[38;5;241m=\u001b[39m clean\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown dataset PROTEINS. See TUDataset.available_datasets() for a complete list ofavailable datasets."
     ]
    }
   ],
   "source": [
    "# Basic information about the dataset is here: https://paperswithcode.com/dataset/proteins\n",
    "# \"clean\" removes isomorphic (non-distinguishable) graphs\n",
    "dataset = spektral.datasets.TUDataset('PROTEINS', clean=True)\n",
    "#dataset = CustomTUDataset('PROTEINS', clean=True)\n",
    "#dataset = spektral.datasets.citation.Citation.available_datasets()\n",
    "#New in version 1.2: sparse was renamed to sparse_output\n",
    "#current version of scikit learn is 1.4.0. I downgraded it to 1.1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95d5790-f494-45d6-877a-b5031891f7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pubmed = spektral.datasets.citation.Citation(\"Pubmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fe0d01-1af5-487c-b511-f90d82e1cc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reddit = spektral.datasets.graphsage.Reddit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8ea090-ab14-41d0-8682-3443e9bdbb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://chrsmrrs.github.io/datasets/docs/datasets/\n",
    "dataset = spektral.datasets.TUDataset('DD', clean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd92d58-69df-4e3c-9262-c815c9c1a82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reddit[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eb2e21-7da7-4b2f-8528-e35a4cfbda63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bab116-caa6-44ea-8f3a-2a06053941f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms are inspired by torchvision-like libraries.\n",
    "# You can apply them using apply (in place) or map (returns a list instead).\n",
    "dataset.apply(T.ClusteringCoeff())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d073f9-57bf-4fd2-8a81-c238514acd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LayerPreprocess allows to apply any preprocessing as suggested by a layer,\n",
    "# in this case adding self-loops and normalizing the adjacency matrix.\n",
    "dataset.apply(\n",
    "    T.LayerPreprocess(layers.GCNConv)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1a3f40-be5a-4f91-bdcd-4d85a43e05a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split by stratifying on the labels\n",
    "train_idx, test_idx = train_test_split(range(len(dataset)), stratify = [g.y for g in dataset])\n",
    "train_dataset = dataset[train_idx]\n",
    "test_dataset = dataset[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f79d277-e46c-4581-97df-84299287d9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In alternative, we can use BatchLoader to create \"classic\" mini-batches by padding\n",
    "# to the size of the largest graph.\n",
    "train_loader = spektral.data.loaders.BatchLoader(train_dataset, batch_size=8)\n",
    "for inputs_alt, labels_alt in train_loader:\n",
    "  print(inputs_alt[0].shape)\n",
    "  print(inputs_alt[1].shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462a2fa1-21ee-456b-a6c9-09c9d6b7c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP1(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_dim = 32, out_dim = 32):\n",
    "        super(MLP1, self).__init__()\n",
    "        self.linear1 = tf.keras.layers.Dense(hidden_dim, activation = \"relu\")\n",
    "        self.linear2 = tf.keras.layers.Dense(hidden_dim, activation = \"relu\")\n",
    "        self.linear3 = tf.keras.layers.Dense(hidden_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        ret = self.linear1(inputs)\n",
    "        ret = self.linear2(ret)\n",
    "        ret = self.linear3(ret)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8772834-d701-47ca-a776-e7f1d8a45efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP2(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_dim = 32, out_dim = 32):\n",
    "        super(MLP2, self).__init__()\n",
    "        self.linear1 = tf.keras.layers.Dense(hidden_dim, activation = \"relu\")\n",
    "        self.linear2 = tf.keras.layers.Dense(hidden_dim, activation = \"relu\")\n",
    "        self.linear3 = tf.keras.layers.Dense(hidden_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        ret = self.linear1(inputs)\n",
    "        ret = self.linear2(ret)\n",
    "        ret = self.linear3(ret)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2657d09-57f5-42e3-84d6-9f6d08501787",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nu_log_initializer (tf.keras.initializers.Initializer):\n",
    "    def __init__ (self):\n",
    "        self.uniform = tf.keras.initializers.RandomUniform(minval = 0, maxval = 1)\n",
    "\n",
    "    def __call__(self, shape, dtype = None, **kwargs):\n",
    "        ret = tf.math.log(tf.math.log(self.uniform(shape))*(-0.5))\n",
    "        return tf.cast(ret, dtype = tf.complex64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9414b4d5-ef76-4d37-959d-3e97590b91b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class theta_log_initializer (tf.keras.initializers.Initializer):\n",
    "    def __init__ (self):\n",
    "        self.uniform = tf.keras.initializers.RandomUniform(minval = 0, maxval = 1)\n",
    "\n",
    "    def __call__(self, shape, dtype = None, **kwargs):\n",
    "        ret = tf.math.log(2*math.pi*self.uniform(shape))\n",
    "        return tf.cast(ret, dtype = tf.complex64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adfb2d0-2c6b-4c3f-8248-66816c99f885",
   "metadata": {},
   "outputs": [],
   "source": [
    "class complex_initializer (tf.keras.initializers.Initializer):\n",
    "    def __init__ (self):\n",
    "        self.real_uniform = tf.keras.initializers.GlorotUniform()\n",
    "        self.imag_uniform = tf.keras.initializers.GlorotUniform()\n",
    "\n",
    "    def __call__(self, shape, dtype = None, **kwargs):\n",
    "        return tf.dtypes.complex(self.real_uniform(shape)/2,self.imag_uniform(shape)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c851a8-9079-4d97-babc-4d3860610cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Nk_computing(adj, K):\n",
    "    K = 5\n",
    "#for now I will use it like this\n",
    "#as soon as possible I will put this in preprocessing, to avoid computing this every time\n",
    "    adjacency_powers = []#in position n,i,j there is a 1 if i and j are at distance n\n",
    "    #is the powers of the adjacency matrix, but it always has a 1 if a value is positive\n",
    "    adjacency_powers.append(adj)\n",
    "    all_previous = [] #I need this to check if there weren't any before ()\n",
    "    #(I will optimize it later, but right now, something will be computed twice if there are\n",
    "    #two paths between two nodes with different lengths)\n",
    "    #at the start it will take the diagonal identity matrix\n",
    "    all_previous.append(tf.eye(adj.shape[0], dtype=tf.int32))\n",
    "    for i in range(K):\n",
    "        to_append = adjacency_powers[-1]@adj\n",
    "        to_append = tf.cast(to_append>0, tf.int32)\n",
    "        adjacency_powers.append(to_append)\n",
    "        to_append_2 = all_previous[-1] + adjacency_powers[-2]\n",
    "        to_append_2 = tf.cast(to_append_2>0, tf.int32)\n",
    "        all_previous.append(to_append_2)\n",
    "    adjacency_powers = tf.convert_to_tensor(adjacency_powers)\n",
    "    all_previous = tf.convert_to_tensor(all_previous)\n",
    "    distances = tf.cast(adjacency_powers>all_previous,tf.int32)\n",
    "    identity = tf.reshape(tf.eye(adj.shape[0],dtype=tf.int32),[1,adj.shape[0],adj.shape[0]])\n",
    "    to_ADD = tf.concat((identity,distances),0)\n",
    "    to_ADD = tf.cast(to_ADD, tf.float32)\n",
    "    return to_ADD\n",
    "#one = MLP1()\n",
    "#two = MLP2()\n",
    "#res = one(nodes)\n",
    "#broadcasted_mat_res = tf.broadcast_to(mat_res,(7,5,5,32))\n",
    "#new_thing = tf.reduce_sum(broadcasted_mat_res * broadcasted_to_ADD, axis = 2)\n",
    "#new_ret = two(new_thing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0917dc43-c614-4cb7-a2a4-be9b263713f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batch_Nk_computing(adj, K):\n",
    "    K = 5\n",
    "    adj = tf.cast(adj>0.0,tf.int32)\n",
    "    #for now I will use it like this\n",
    "    #as soon as possible I will put this in preprocessing, to avoid computing this every time\n",
    "    adjacency_powers = []#in position n,i,j there is a 1 if i and j are at distance n\n",
    "    #is the powers of the adjacency matrix, but it always has a 1 if a value is positive\n",
    "    adjacency_powers.append(adj)\n",
    "    all_previous = [] #I need this to check if there weren't any before ()\n",
    "    #(I will optimize it later, but right now, something will be computed twice if there are\n",
    "    #two paths between two nodes with different lengths)\n",
    "    #at the start it will take the diagonal identity matrix\n",
    "    #all_previous.append(tf.eye(adj.shape[0], dtype=tf.int32))\n",
    "    all_previous.append(tf.eye(adj.shape[-1], batch_shape = [adj.shape[0]], dtype=tf.int32))\n",
    "    for i in range(K):\n",
    "        to_append = adjacency_powers[-1]@adj\n",
    "        to_append = tf.cast(to_append>0, tf.int32)\n",
    "        adjacency_powers.append(to_append)\n",
    "        to_append_2 = all_previous[-1] + adjacency_powers[-2]\n",
    "        to_append_2 = tf.cast(to_append_2>0, tf.int32)\n",
    "        all_previous.append(to_append_2)\n",
    "    adjacency_powers = tf.convert_to_tensor(adjacency_powers)\n",
    "    all_previous = tf.convert_to_tensor(all_previous)\n",
    "    distances = tf.cast(adjacency_powers>all_previous,tf.int32)\n",
    "    identity = tf.eye(adj.shape[-1],batch_shape = [1,adj.shape[0]],dtype=tf.int32)\n",
    "    #tf.broadcast_to(identity, (adj.shape[0],1,adj.shape[-1], adj.shape[-1]))\n",
    "    to_ADD = tf.concat((identity,distances),0)\n",
    "    to_ADD = tf.cast(to_ADD, tf.float32)\n",
    "    return to_ADD\n",
    "#one = MLP1()\n",
    "#two = MLP2()\n",
    "#res = one(nodes)\n",
    "#broadcasted_mat_res = tf.broadcast_to(mat_res,(7,5,5,32))\n",
    "#new_thing = tf.reduce_sum(broadcasted_mat_res * broadcasted_to_ADD, axis = 2)\n",
    "#new_ret = two(new_thing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662c0e90-2e1a-4a04-a26c-dc365bc6f2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADD(mlp1, mlp2, nodes, to_add):\n",
    "    ret = mlp1(nodes)\n",
    "    reshaped_to_add = tf.reshape(to_add, to_add.shape+[1])\n",
    "    broadcasted_to_add = tf.broadcast_to(reshaped_to_add,to_add.shape+[nodes.shape[-1]])\n",
    "    broadcasted_nodes = tf.broadcast_to(nodes,\n",
    "                                       (to_add.shape[0],nodes.shape[0],nodes.shape[1],nodes.shape[2]))\n",
    "    broadcasted_nodes = tf.reshape(broadcasted_nodes,\n",
    "                                  (to_add.shape[0],nodes.shape[0],nodes.shape[1],1,nodes.shape[2]))\n",
    "    broadcasted_nodes = tf.broadcast_to(broadcasted_nodes,(to_add.shape[0],nodes.shape[0],nodes.shape[1],nodes.shape[1],nodes.shape[2]))\n",
    "    ret = tf.reduce_sum(broadcasted_to_add * broadcasted_nodes, axis = 3)\n",
    "    #check if axis = 3 is right. it might be axis = 2\n",
    "    ret = mlp2(ret)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da61e95-4f03-41de-9a69-3ec5aa02ff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_GRED_layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, out_dimension = 32, K = 5, x_dim = 32, state_dim = 32):\n",
    "        super(my_GRED_layer, self).__init__()\n",
    "        self.out_dimension = out_dimension\n",
    "        self.K = K\n",
    "        self.x_dim = x_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.linear1 = MLP1()\n",
    "        self.linear2 = MLP2()\n",
    "        self.linear3 = MLP2()\n",
    "        self.log_nu = self.add_weight(shape = [state_dim], initializer = nu_log_initializer(), dtype = tf.complex64)\n",
    "        self.log_theta =self.add_weight(shape = [state_dim], initializer = theta_log_initializer(), dtype = tf.complex64)\n",
    "        self.W_in = self.add_weight(shape = (state_dim,x_dim), initializer = complex_initializer(), dtype = tf.complex64)\n",
    "        self.W_out = self.add_weight(shape = (x_dim, state_dim), initializer = complex_initializer(), dtype = tf.complex64)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        h,a = inputs\n",
    "        to_ADD = batch_Nk_computing(a, self.K)\n",
    "        #for now I will compute everything on the spot. Later I will preprocess this\n",
    "        xs = ADD(self.linear1, self.linear2, h, to_ADD)\n",
    "        lambda_1 = tf.math.exp((-1)*tf.math.exp(self.log_nu) + 1j*tf.math.exp(self.log_theta))\n",
    "        #optimal multiplication by diagonal matrix is  matrix * tf.transpose([vector]) if the diagonal matrix is on the left\n",
    "        #matrix * tf.transpose([vector]) == tf.linalg.diag(vector)@matrix\n",
    "        #matrix*vector == matrix@tf.linalg.diag(vector)\n",
    "        broadcasted_lambda_1 = tf.broadcast_to(lambda_1, (xs.shape[0],xs.shape[1], self.state_dim))\n",
    "        #I will probably need to cast something to complex values\n",
    "        broadcasted_lambda_1 = tf.broadcast_to(lambda_1, (xs.shape[0],xs.shape[1],self.state_dim))\n",
    "        broadcasted_w_in = tf.broadcast_to(self.W_in, (xs.shape[0],xs.shape[1],self.state_dim,self.x_dim))#i can optimize this by adding the batch dimension later\n",
    "        powers = tf.transpose(tf.broadcast_to(tf.range(xs.shape[0], dtype = tf.float32),(self.state_dim,xs.shape[1],xs.shape[0])),[2,1,0])\n",
    "        powers = tf.cast(powers, dtype = tf.complex64)\n",
    "        lambda_powers = broadcasted_lambda_1 ** powers\n",
    "        transposed_lambda_powers = tf.transpose(tf.reshape(lambda_powers,[xs.shape[0],xs.shape[1],1,self.state_dim]),[0,1,3,2])\n",
    "        lambda_times_w_in = transposed_lambda_powers * broadcasted_w_in\n",
    "        #(tf.transpose([lambda_1])*w_in)@tf.reshape(xs[0,0,0],[32,1])\n",
    "        xs = tf.cast(xs, dtype = tf.complex64)\n",
    "        broadcasted_lambda_times_w_in = tf.broadcast_to(\n",
    "            tf.reshape(lambda_times_w_in,[xs.shape[0],xs.shape[1],1,self.state_dim,self.x_dim]),\n",
    "            [xs.shape[0],xs.shape[1],xs.shape[2],self.state_dim,self.x_dim])\n",
    "        s_K = broadcasted_lambda_times_w_in @ tf.reshape(xs,\n",
    "                                                         [xs.shape[0],xs.shape[1],xs.shape[2],self.x_dim,1])\n",
    "        s_K = tf.reduce_sum(tf.reshape(s_K,[xs.shape[0],xs.shape[1],xs.shape[2],self.state_dim]),0)\n",
    "        #now is just ,multiply by w_out, take the real part and put in the last MLP\n",
    "        broadcasted_w_out = tf.broadcast_to(self.W_out, [xs.shape[1],xs.shape[2],self.x_dim,self.state_dim])\n",
    "        reshaped_s_K = tf.reshape(s_K,[xs.shape[1],xs.shape[2],self.state_dim,1])\n",
    "        output = tf.reshape(broadcasted_w_out@reshaped_s_K,[xs.shape[1],xs.shape[2],self.x_dim])\n",
    "        output = tf.math.real(output)\n",
    "        output = self.linear3(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001b1e74-b30b-4589-b329-4df74208ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_GRED_model(tf.keras.Model):\n",
    "    def __init__(self, out_dim = 2):\n",
    "        super(my_GRED_model, self).__init__()\n",
    "        self.out_dim = out_dim\n",
    "        self.gred_1 = my_GRED_layer()\n",
    "        self.out_linear = tf.keras.layers.Dense(out_dim)\n",
    "        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        ret = self.gred_1(inputs)\n",
    "        ret = tf.reduce_mean(ret,1)\n",
    "        ret = self.out_linear(ret)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cae9f8-6d1d-4bd3-8cb4-0c825c3b5d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = my_GRED_model()\n",
    "mod.compile(\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[tf.keras.metrics.CategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8431d19c-41b6-4876-a84d-00dd2e51fc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mod.fit(train_loader, epochs=10, steps_per_epoch=train_loader.steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65faa48f-664d-43c0-b308-ac40a5c5a1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = tf.keras.optimizers.experimental.SGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e221f15f-8628-4584-a154-780f4884a499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function(reduce_retracing=True)\n",
    "def train_step(batch):\n",
    "  \n",
    "        xb, yb = batch\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Get the predictions of the model\n",
    "            y_predicted = mod(xb)\n",
    "\n",
    "            # Compute the average loss of the predictions\n",
    "            ce = tf.keras.losses.CategoricalCrossentropy()(yb, y_predicted)\n",
    "            loss = ce\n",
    "\n",
    "        # Get the gradients of the parameters\n",
    "        grads = tape.gradient(loss, mod.trainable_variables)\n",
    "\n",
    "        # Update the parameters using gradient descent\n",
    "        sgd.apply_gradients(zip(grads, mod.trainable_variables))\n",
    "\n",
    "        return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82fed22-1fe1-49fc-b059-94733c3d7948",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for i in range(epochs):\n",
    "    count = 0\n",
    "    for batch in train_loader:\n",
    "        if count >= train_loader.steps_per_epoch:\n",
    "            break\n",
    "        train_step(batch)\n",
    "        count+=1\n",
    "        if count == 90:\n",
    "            print(count)\n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fc7aac-92f6-499e-9ac2-b8dabc099f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I have no clue why the outputs are the opposite of what it should be, check the loss function\n",
    "#also, every time it runs, since there are different tensor shapes, retracing makes a mess.\n",
    "#reduce_retracing = True should do exactly this, but it will see exactly how to do it\n",
    "#also, check why fit does not work and check that all calculations are correct in the other one\n",
    "#especially when computing the last probability\n",
    "#import os\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "#this should turn off the warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d52fb7-f423-4311-b973-d11e2a63b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.keras.activations.softmax((-1)*mod(batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab7e52a-2b4f-4b56-99ec-8dc9a0902c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4827442e-7d1a-4e10-bdde-c9e87ca5a7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be3d441-d4e7-4f79-adb4-5ccbd293f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_mean(tf.cast((a[:,0]>a[:,1]) == (batch[1][:,0] > batch[1][:,1]),tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df269e7c-b73e-40a7-a6bd-a6775131c1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7b5569-e5b3-401a-beae-effb4afb9ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.losses.BinaryCrossentropy()((-1)*mod(batch[0]),batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11899d71-7240-43ec-87ae-843e4d2f5757",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.metrics.Accuracy()((-1)*mod(batch[0]),batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1676b57a-e691-4b9f-af1e-9ad94276bed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (-1)*mod(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481b65fa-5761-4f49-9aa7-c9772bb51037",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.metrics.Accuracy()(a,batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dff0ac-20b8-4700-8a28-16a94d74a17f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "xs = tf.random.uniform((7,8,12,32))\n",
    "w_in = tf.random.uniform((64,32))\n",
    "w_out = tf.random.uniform((32,64))\n",
    "lambda_1 = tf.random.uniform([64])\n",
    "broadcasted_lambda_1 = tf.broadcast_to(lambda_1, (7,8,64))\n",
    "broadcasted_w_in = tf.broadcast_to(w_in, (7,8,64,32))#i can optimize this by adding the batch dimension later\n",
    "powers = tf.transpose(tf.broadcast_to(tf.range(7, dtype = tf.float32),(64,8,7)),[2,1,0])\n",
    "lambda_powers = broadcasted_lambda_1 ** powers\n",
    "transposed_lambda_powers = tf.transpose(tf.reshape(lambda_powers,[7,8,1,64]),[0,1,3,2])\n",
    "lambda_times_w_in = transposed_lambda_powers * broadcasted_w_in\n",
    "#(tf.transpose([lambda_1])*w_in)@tf.reshape(xs[0,0,0],[32,1])\n",
    "\n",
    "broadcasted_lambda_times_w_in = tf.broadcast_to(tf.reshape(lambda_times_w_in,[7,8,1,64,32]),[7,8,12,64,32])\n",
    "#tf.reshape(broadcasted_lambda_times_w_in @ tf.reshape(xs,[7,8,12,32,1]),[7,8,12,64])[1,0]\n",
    "#tf.transpose((tf.transpose([lambda_1])*w_in)@tf.reshape(xs[0,0,0],[32,1]))\n",
    "s_K = broadcasted_lambda_times_w_in @ tf.reshape(xs,[7,8,12,32,1])\n",
    "s_K = tf.reduce_sum(tf.reshape(s_K,[7,8,12,64]),0)\n",
    "#now is just ,multiply by w_out, take the real part and put in the last MLP\n",
    "s_K.shape\n",
    "broadcasted_w_out = tf.broadcast_to(w_out, [8,12,32,64])\n",
    "reshaped_s_K = tf.reshape(s_K,[8,12,64,1])\n",
    "output = tf.reshape(broadcasted_w_out@reshaped_s_K,[8,12,32])\n",
    "output = tf.math.real(output)\n",
    "output = MLP2()(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833dadc9-cbf1-40e2-89de-eda09ad72c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2bdf74-8ff6-4d87-9844-097444d6900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4707a55-443d-467a-870c-817ee58462d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
